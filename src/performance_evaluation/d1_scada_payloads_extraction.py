#!/usr/bin/env python3
"""
Payload Extraction by Period Module

This module extracts data payloads from test PCAP files based on periodic patterns
identified by a2_period_identifier.py. It matches the same periodic patterns in
test data and extracts corresponding data payloads.

Author: PVParser Project
Creation Date: 2025-01-27
Version: 1.0.0
"""

import os
import sys
import csv
import json
import glob
import ast
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from collections import Counter
import pandas as pd
from scapy.all import rdpcap, IP, TCP, Raw

# Add the parent directory to the path to enable imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from protocol_field_inference.b2_data_payloads_extraction import PacketProcessor
from basis.protocol_factory import ProtocolFactory
from basis.ics_basis import is_ics_port_by_protocol, ENIP, MMS, MODBUS
from period_identification.control_code_enip import extract_enip_control_code
from period_identification.control_code_modbus import extract_modbus_control_code
from period_identification.control_code_mms import extract_mms_control_code


class PeriodBasedPayloadExtractor:
    """
    Extract data payloads from test PCAP files based on identified periodic patterns.
    """
    
    def __init__(self, protocol_type: str = None):
        """
        Initialize the extractor with a specific protocol.
        
        Args:
            protocol_type: Protocol type for data extraction (enip, modbus)
        """
        self.protocol_type = protocol_type
        self.packet_processor = PacketProcessor(protocol_type)
        # For evaluation-time pure data extraction
        self.protocol_factory = ProtocolFactory()
        self.protocol_factory.set_protocol(protocol_type)
        
    def load_period_patterns(self, period_results_json: str) -> Dict[str, Dict[str, Any]]:
        """
        Load periodic patterns from JSON file generated by a2_period_identifier.py.
        
        Args:
            period_results_json: Path to the period results JSON file
            
        Returns:
            Dictionary mapping session keys to their detailed period information
        """
        if not os.path.exists(period_results_json):
            print(f"Period results JSON file not found: {period_results_json}")
            return {}
        
        with open(period_results_json, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Extract detailed period info
        detailed_period_info = data.get('detailed_period_info')
        
        print(f"Loaded period information for {len(detailed_period_info)} sessions")
        
        # Print summary
        # for session_key, info in detailed_period_info.items():
        #     print(f"  Session {session_key}: period={info.get('period')}, pattern={info.get('period_pattern')}")
        
        return detailed_period_info
    
    def extract_payloads_by_session(self, test_pcap_folder: str, session_key: str, 
                                  session_info: Dict[str, Any], output_folder: str = None) -> List[Dict[str, Any]]:
        """
        Extract data payloads from test PCAP folder based on session period information.
        
        Args:
            test_pcap_folder: Path to folder containing test PCAP files
            session_key: Session key for identification
            session_info: Session period information from JSON
            output_folder: Optional output folder for saving results
            
        Returns:
            List of extracted data payloads with timestamps grouped by period
        """
        # Prepare expected pattern
        period_pattern_str = session_info.get('period_pattern')
        if not period_pattern_str:
            print(f"Warning: No period_pattern for session {session_key}")
            return []
        expected_pattern = [p.strip() for p in period_pattern_str.split(',') if p and p.strip()]
        if not expected_pattern:
            print(f"Warning: Empty period_pattern for session {session_key}")
            return []

        # Parse session key for IPs
        ip1, ip2 = self._parse_session_key_ips(session_key)
        if not ip1 or not ip2:
            print(f"Warning: Could not parse IPs from session_key {session_key}")
            return []

        # Find all PCAP files in the folder
        pcap_files = glob.glob(os.path.join(test_pcap_folder, "*.pcap"))
        if not pcap_files:
            print(f"Warning: No PCAP files found in {test_pcap_folder}")
            return []

        print(f"Processing session {session_key} from {len(pcap_files)} PCAP files...")

        # collect segments per pcap, then sort globally by segment start time
        collected_segments: List[Tuple[float, List[str]]] = []  # (segment_start_ts, payloads_hex)

        # Process each PCAP file in the folder
        for pcap_file in pcap_files:
            try:
                packets = rdpcap(pcap_file)
            except Exception as e:
                print(f"Warning: Failed to read pcap {pcap_file}: {e}")
                continue

            # Build representations for packets matching the IP pair (both directions)
            total_packets = 0
            packet_reps: List[Tuple[int, str, Optional[bytes]]] = []  # (index, packet_rep, pure_data)
            packet_times: List[float] = []
            for idx, pkt in enumerate(packets):
                total_packets += 1
                # Filter out packets without IP and TCP layers
                if not pkt.haslayer(IP) or not pkt.haslayer(TCP):
                    continue
                
                # Filter out packets not between the two IPs
                src = pkt[IP].src
                dst = pkt[IP].dst
                if not ((src == ip1 and dst == ip2) or (src == ip2 and dst == ip1)):
                    continue
                
                # Filter out packets without application-layer payloads
                if Raw not in pkt or len(pkt[Raw].load) <= 4:
                    continue

                # Use protocol factory to extract pure data for evaluation
                payload_bytes: Optional[bytes] = None
                try:
                    _, pure_data = self.protocol_factory.extract_pure_data(pkt)
                    if pure_data:
                        payload_bytes = pure_data
                except Exception:
                    payload_bytes = None
                packet_rep = self._build_packet_representation(pkt, ip1, ip2)
                if packet_rep is not None:
                    packet_reps.append((idx, packet_rep, payload_bytes))
                    # capture timestamp aligned to reps list
                    packet_times.append(pkt.time)

            if not packet_reps:
                continue

            print(f"  Total packets: {total_packets}, Extracted packets: {len(packet_reps)}")

            # Sequential matching within this pcap only; record each full segment with its start time
            k = 0  # index into expected_pattern
            current_segment_payloads: List[str] = []
            current_segment_start_ts: Optional[float] = None
            for i_in_file, (_, packet_rep, payload) in enumerate(packet_reps):
                if packet_rep == expected_pattern[k]:
                    if k == 0:
                        current_segment_start_ts = packet_times[i_in_file]
                    if payload:
                        current_segment_payloads.append(payload.hex())
                    k += 1
                    if k == len(expected_pattern):
                        if current_segment_payloads and current_segment_start_ts is not None:
                            collected_segments.append((current_segment_start_ts, current_segment_payloads))
                        # reset to look for next sequence
                        k = 0
                        current_segment_payloads = []
                        current_segment_start_ts = None
                else:
                    # mismatch: reset and check if this packet can be a new start
                    k = 0
                    current_segment_payloads = []
                    current_segment_start_ts = None
                    if packet_rep == expected_pattern[k]:
                        current_segment_start_ts = packet_times[i_in_file]
                        if payload:
                            current_segment_payloads.append(payload.hex())
                        k = 1

        # sort segments across all pcaps by their start time
        collected_segments.sort(key=lambda s: s[0])
        
        # Convert to list of dictionaries with timestamp and payloads
        all_matched_segments_data: List[Dict[str, Any]] = []
        for start_ts, payloads in collected_segments:
            segment_data = {
                "timestamp": float(start_ts),  # Ensure timestamp is a standard float
                "payloads": payloads
            }
            all_matched_segments_data.append(segment_data)

        if output_folder and all_matched_segments_data:
            self._save_extraction_results(test_pcap_folder, session_key, session_info, all_matched_segments_data, output_folder)

        print(f"  Extracted {len(all_matched_segments_data)} payload groups for session {session_key}")
        return all_matched_segments_data

    def _parse_session_key_ips(self, session_key: str) -> Tuple[Optional[str], Optional[str]]:
        """Parse a session_key string like "('192.168.1.10', '192.168.1.99', 6)" to get (ip1, ip2)."""
        try:
            tpl = ast.literal_eval(session_key)
            if isinstance(tpl, tuple) and len(tpl) >= 2 and isinstance(tpl[0], str) and isinstance(tpl[1], str):
                return tpl[0], tpl[1]
        except Exception:
            return None, None
        return None, None

    def _build_packet_representation(self, packet, ip1: str, ip2: str) -> Optional[str]:
        """Match the representation format from extract_directed_length_sequence_with_control.

        Format: "{direction}-{packet_length}-{control_code}"
        - direction: 'C' if dport is ICS port, 'S' if sport is ICS port; fallback by src ip vs ip1/ip2
        - packet_length: len(packet)
        - control_code: from ENIP/MODBUS extractor
        """
        if not packet.haslayer(IP) or not packet.haslayer(TCP):
            return None
        tcp = packet[TCP]
        # Determine direction by port first (consistent with extractor)
        direction = None
        if is_ics_port_by_protocol(tcp.dport, self.protocol_type):
            direction = 'C'
        elif is_ics_port_by_protocol(tcp.sport, self.protocol_type):
            direction = 'S'
        else:
            # Fallback: use IP role relative to session
            direction = 'C' if packet[IP].src == ip1 else 'S'

        pkt_len = len(packet)
        control_code = None
        if self.protocol_type == MODBUS:
            control_code = extract_modbus_control_code(packet)
        elif self.protocol_type == ENIP:
            control_code = extract_enip_control_code(packet)
        elif self.protocol_type == MMS:
            control_code = extract_mms_control_code(packet)
        # Ensure control_code uses a non-colliding token when missing
        # Align with extractor behavior where None becomes 'None' in f-strings
        control_code_str = str(control_code) if control_code is not None else 'None'
        return f"{direction}-{pkt_len}-{control_code_str}"

    def _save_extraction_results(self, test_pcap_folder: str, session_key: str, 
                               session_info: Dict[str, Any], period_data_with_timestamps: List[Dict[str, Any]], 
                               output_folder: str):
        """
        Save extraction results to JSON file with timestamp information.
        
        Args:
            test_pcap_folder: Path to test PCAP folder
            session_key: Session key for identification
            session_info: Session information used for extraction
            period_data_with_timestamps: Extracted payloads with timestamps grouped by period
            output_folder: Output folder path
        """
        os.makedirs(output_folder, exist_ok=True)
        
        # Generate output filename in the same format as data_payloads_combination
        period = session_info.get('period')
        output_file = os.path.join(output_folder, f"{session_key}_data_payloads.json")
        
        # Prepare results data with timestamp information
        results_data = {
            "metadata": {
                "pcap_folder": test_pcap_folder,
                # "session_key": session_key,
                "period": period,
                "protocol": self.protocol_type
            },
            "period_payloads_with_timestamps": period_data_with_timestamps
        }
        
        # Save to JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"Extraction results saved to: {output_file}")
        print(f"  Session: {session_key}, Period: {period}, Payload groups: {len(period_data_with_timestamps)}")
    
    def extract_by_session_keys(self, test_pcap_folder: str, session_keys: List[str], 
                               period_results_json: str, output_folder: str = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        Extract payloads for specified session keys from PCAP folder.
        
        Args:
            test_pcap_folder: Path to folder containing test PCAP files
            session_keys: List of session keys to extract
            period_results_json: Path to period results JSON file
            output_folder: Output folder for saving results
            
        Returns:
            Dictionary mapping session keys to their extracted payloads with timestamps
        """
        # Load period patterns from JSON
        session_patterns = self.load_period_patterns(period_results_json)
        
        results = {}
        
        for session_key in session_keys:
            if session_key not in session_patterns:
                print(f"Warning: Session {session_key} not found in period results")
                results[session_key] = []
                continue
            
            session_info = session_patterns[session_key]
            print(f"Extracting payloads for session: {session_key}")
            
            # Extract payloads for this session
            payloads = self.extract_payloads_by_session(test_pcap_folder, session_key, session_info, output_folder)
            results[session_key] = payloads
        
        return results
    
    def analyze_payloads_data_volume(self, data_payloads_folder: str) -> Dict[str, Dict[str, Any]]:
        """
        Analyze data volume of payloads files in the data_payloads_combination folder.
        
        Args:
            data_payloads_folder: Path to data_payloads_combination folder
            
        Returns:
            Dictionary containing analysis results for each payload file
        """
        analysis_results = {}
        
        if not os.path.exists(data_payloads_folder):
            print(f"Data payloads folder not found: {data_payloads_folder}")
            return analysis_results
        
        # Find all JSON files in the folder
        json_files = glob.glob(os.path.join(data_payloads_folder, "*.json"))
        
        if not json_files:
            print(f"No JSON files found in {data_payloads_folder}")
            return analysis_results
        
        print(f"Analyzing {len(json_files)} payload files...")
        
        for json_file in json_files:
            try:
                # Extract session key from filename
                filename = os.path.basename(json_file)
                session_key = filename.replace("_data_payloads.json", "")
                
                # Load JSON data
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Extract payload data
                if 'period_payloads_with_timestamps' in data:
                    payloads_data = data['period_payloads_with_timestamps']
                    # Extract payloads from the new format
                    payloads = []
                    for segment_data in payloads_data:
                        if isinstance(segment_data, dict) and 'payloads' in segment_data:
                            payloads.append(segment_data['payloads'])
                        elif isinstance(segment_data, list):
                            payloads.append(segment_data)
                else:
                    payloads = data
                
                # Calculate statistics
                total_segments = len(payloads) if isinstance(payloads, list) else 0
                total_bytes = 0
                payload_lengths = []
                segment_sizes = []
                
                if isinstance(payloads, list):
                    for segment in payloads:
                        if isinstance(segment, list):
                            segment_size = len(segment)
                            segment_sizes.append(segment_size)
                            
                            for payload_hex in segment:
                                if isinstance(payload_hex, str):
                                    try:
                                        payload_bytes = bytes.fromhex(payload_hex)
                                        payload_length = len(payload_bytes)
                                        payload_lengths.append(payload_length)
                                        total_bytes += payload_length
                                    except ValueError:
                                        continue
                        elif isinstance(segment, str):
                            # Single payload string
                            try:
                                payload_bytes = bytes.fromhex(segment)
                                payload_length = len(payload_bytes)
                                payload_lengths.append(payload_length)
                                total_bytes += payload_length
                                segment_sizes.append(1)
                            except ValueError:
                                continue
                
                # Calculate statistics
                avg_payload_length = sum(payload_lengths) / len(payload_lengths) if payload_lengths else 0
                avg_segment_size = sum(segment_sizes) / len(segment_sizes) if segment_sizes else 0
                min_payload_length = min(payload_lengths) if payload_lengths else 0
                max_payload_length = max(payload_lengths) if payload_lengths else 0
                min_segment_size = min(segment_sizes) if segment_sizes else 0
                max_segment_size = max(segment_sizes) if segment_sizes else 0
                
                # Store analysis results
                analysis_results[session_key] = {
                    'file_path': json_file,
                    'total_segments': total_segments,
                    'total_bytes': total_bytes,
                    'avg_payload_length': round(avg_payload_length, 2),
                    'min_payload_length': min_payload_length,
                    'max_payload_length': max_payload_length,
                    'avg_segment_size': round(avg_segment_size, 2),
                    'min_segment_size': min_segment_size,
                    'max_segment_size': max_segment_size,
                    'payload_length_distribution': dict(Counter(payload_lengths)),
                    'segment_size_distribution': dict(Counter(segment_sizes))
                }
                
                print(f"  {session_key}: {total_segments} segments, {total_bytes} bytes")
                
            except Exception as e:
                print(f"Error analyzing {json_file}: {e}")
                analysis_results[os.path.basename(json_file)] = {
                    'error': str(e),
                    'file_path': json_file
                }
        
        return analysis_results
    
    def print_analysis_summary(self, analysis_results: Dict[str, Dict[str, Any]]):
        """
        Print a summary of the payload analysis results.
        
        Args:
            analysis_results: Results from analyze_payloads_data_volume
        """
        if not analysis_results:
            print("No analysis results to display")
            return
        
        print("\n" + "="*80)
        print("PAYLOAD DATA VOLUME ANALYSIS SUMMARY")
        print("="*80)
        
        # Overall statistics
        total_files = len(analysis_results)
        successful_files = sum(1 for result in analysis_results.values() if 'error' not in result)
        total_segments = sum(result.get('total_segments', 0) for result in analysis_results.values() if 'error' not in result)
        total_bytes = sum(result.get('total_bytes', 0) for result in analysis_results.values() if 'error' not in result)
        
        print(f"Files analyzed: {total_files}")
        print(f"Successful: {successful_files}")
        print(f"Failed: {total_files - successful_files}")
        print(f"Total segments: {total_segments}")
        print(f"Total bytes: {total_bytes:,}")
        
        if total_segments > 0:
            print(f"Average bytes per segment: {total_bytes / total_segments:.2f}")
        
        print("\n" + "-"*80)
        print("DETAILED RESULTS BY SESSION")
        print("-"*80)
        
        for session_key, result in analysis_results.items():
            if 'error' in result:
                print(f"{session_key}: ERROR - {result['error']}")
                continue
            
            print(f"\nSession: {session_key}")
            print(f"  Segments: {result['total_segments']}")
            print(f"  Total bytes: {result['total_bytes']:,}")
            print(f"  Avg payload length: {result['avg_payload_length']} bytes")
            print(f"  Payload length range: {result['min_payload_length']} - {result['max_payload_length']} bytes")
            print(f"  Avg segment size: {result['avg_segment_size']} items")
            print(f"  Segment size range: {result['min_segment_size']} - {result['max_segment_size']} items")
            
            # Show most common payload lengths
            if result['payload_length_distribution']:
                most_common_lengths = sorted(result['payload_length_distribution'].items(), 
                                           key=lambda x: x[1], reverse=True)[:5]
                print(f"  Most common payload lengths: {dict(most_common_lengths)}")
            
            # Show most common segment sizes
            if result['segment_size_distribution']:
                most_common_sizes = sorted(result['segment_size_distribution'].items(), 
                                         key=lambda x: x[1], reverse=True)[:5]
                print(f"  Most common segment sizes: {dict(most_common_sizes)}")
        
        print("\n" + "="*80)


def main():
    
    # swat dataset Configuration
    # dataset_name = "swat"
    # protocol_type = ENIP
    # period_results_folder = "Dec2019_00003_20191206104500"
    # data_payloads_folder = "Dec2019_00013_20191206131500"
    # scada_ip = "192.168.1.200"
    # session_keys = [f"('192.168.1.10', '{scada_ip}', 6)", f"('192.168.1.20', '{scada_ip}', 6)", f"('{scada_ip}', '192.168.1.30', 6)", 
    #     f"('{scada_ip}', '192.168.1.40', 6)", f"('{scada_ip}', '192.168.1.50', 6)", f"('{scada_ip}', '192.168.1.60', 6)"]
    
    
    dataset_name = "wadi_enip"
    protocol_type = ENIP
    period_results_folder = "wadi_capture_00043_00047"
    data_payloads_folder = "wadi_capture_00087_00091"
    scada_ip = "192.168.1.67"
    session_keys = [f"('192.168.1.53', '{scada_ip}', 6)", f"('192.168.1.3', '{scada_ip}', 6)", f"('192.168.1.13', '{scada_ip}', 6)"]
    
    period_results_json = f"src/data/period_identification/{dataset_name}/results/{period_results_folder}/{dataset_name}_period_results.json"  # from training data
    pcap_folder = f"dataset/{dataset_name}/network/evaluation_filtered/{data_payloads_folder}"  # raw pcap files
    output_folder = f"src/data_evaluation/protocol_field_inference/{dataset_name}/data_payloads_extraction/{data_payloads_folder}"  # for evaluation data

    extractor = PeriodBasedPayloadExtractor(protocol_type=protocol_type)
    results = extractor.extract_by_session_keys(pcap_folder, session_keys, period_results_json, output_folder)
    
    # Analyze data volume of extracted payloads
    print("\n" + "="*60)
    print("ANALYZING EXTRACTED PAYLOADS DATA VOLUME")
    print("="*60)
    analysis_results = extractor.analyze_payloads_data_volume(output_folder)
    extractor.print_analysis_summary(analysis_results)


if __name__ == "__main__":
    # Run the main function for all sessions
    main()
    